# -*- coding: utf-8 -*-
"""submission-akhir-najwa-salsabila.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13mnLK_PQRssD-0BIe9_ZscqydDuHhD7W

# Proyek Klasifikasi Gambar: Alzheimer MRI
- **Nama:** Najwa Salsabila
- **Email:** najwas.for.study@gmail.com
- **ID Dicoding:** najwasalsabila

## Import Semua Packages/Library yang Digunakan
"""

!pip install tensorflowjs

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
import cv2
import tensorflow as tf
import keras
from tqdm import tqdm
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow import keras
from tensorflow.keras import regularizers
from keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import glob
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

"""## Data Preparation

### Data Loading
"""

import kagglehub
import os

# == DOWNLOAD DATASET ==
path = kagglehub.dataset_download("uraninjo/augmented-alzheimer-mri-dataset")

print("Path to dataset files:", path)
files = os.listdir(path)
print("Files in dataset folder:", files)

file_path = os.path.join(path, 'AugmentedAlzheimerDataset')
image_data = file_path

"""### Data Preprocessing

#### Split Dataset
"""

train_files = [i for i in glob.glob(image_data + '//*//*')]
np.random.shuffle(train_files)
train_labels = [os.path.dirname(i).split('/')[-1] for i in train_files]
train_data = zip(train_files, train_labels)
train_df = pd.DataFrame(train_data, columns=["Image", 'Label'])
train_df.head()

batch_size = 8
target_size = (224, 224)
test_split = 0.2

all_data = tf.keras.preprocessing.image_dataset_from_directory(
    image_data,
    image_size=target_size,
    batch_size=batch_size
)

dataset_size = tf.data.experimental.cardinality(all_data).numpy()

train_size = int(dataset_size * (1 - test_split))
test_size = dataset_size - train_size

train = all_data.take(train_size)
remaining_data = all_data.skip(train_size)

test = remaining_data.take(test_size)

"""## Modelling"""

sizes = [train_size, test_size]
labels = ['Train', 'Test']
colors = ['#66c2a5', '#fc8d62']

plt.figure(figsize=(5, 5))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%')
plt.title('Dataset Split distribution')
plt.axis('equal')
plt.show()

class_names = all_data.class_names

plt.figure(figsize=(15, 20))
num_images_per_class = 4
count = {class_name : 0 for class_name in class_names}

for images, labels in train.take(1):
    print(images.shape)
    print(labels.shape)
    for i in range(len(labels)):
        label = class_names[labels[i]]

        if count[label] < num_images_per_class:
            ax = plt.subplot(8, 4, sum(count.values()) + 1)
            plt.imshow(images[i].numpy().astype('uint8'))
            plt.title(label)
            plt.axis('off')
            count[label] += 1

        if sum(count.values()) >= num_images_per_class * len(class_names):
            break

all_data.class_names

keras_model = keras.models.Sequential([
    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(64, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.Conv2D(128, (3, 3), activation='relu'),
    keras.layers.MaxPooling2D((2, 2)),
    keras.layers.GlobalAveragePooling2D(),
    keras.layers.Dense(128, activation='relu'),
    keras.layers.Dropout(0.5),
    keras.layers.Dense(4, activation='softmax')
])

keras_model.summary()

"""## Evaluasi dan Visualisasi"""

checkpoint = ModelCheckpoint('model.keras', save_best_only=True)

keras_model.compile(
    optimizer='Adam',
    loss='sparse_categorical_crossentropy',
    metrics = ['accuracy']
)

history = keras_model.fit(
    train,
    epochs=50,
    validation_data=test,
    callbacks=[checkpoint]
)

keras_model.save("model.keras")

keras_model.save("model.h5")

sns.set(style='whitegrid')
fig, axs = plt.subplots(2, figsize=(12, 12))

axs[0].plot(history.history['loss'], label='Train Loss', color='blue')
axs[0].plot(history.history['val_loss'], label='Validation Loss', color='orange')
axs[0].set_title('Loss over epochs')
axs[0].set_xlabel('Epochs')
axs[0].set_ylabel('Losses')
axs[0].legend()
axs[0].grid(True)

axs[1].plot(history.history['accuracy'], label='Train Accuracy', color='green')
axs[1].plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')
axs[1].set_title("Accuracy over epochs")
axs[1].set_xlabel('Epochs')
axs[1].set_ylabel('Accuracies')
axs[1].legend()
axs[1].grid(True)

"""## Konversi Model"""

import tensorflow as tf

# Load the best Keras model
model = tf.keras.models.load_model('model.keras')

save_path = 'mymodel/'
tf.saved_model.save(model, save_path)

keras_model = tf.keras.models.load_model('/content/model.keras')
converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
tflite_model = converter.convert()
with open('tflite_model.tflite', 'wb') as f:
  f.write(tflite_model)

labels = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']
with open('label.txt', 'w') as f:
    for label in labels:
        f.write(label + '\n')

# !tensorflowjs_converter --input_format=keras /content/model.keras tfjs_model
!tfjs.converters.save_keras_model(/content/model.keras, tfjs_model)

"""## Inference (Optional)"""

from tensorflow.keras.preprocessing.image import load_img, img_to_array
from google.colab import files
from IPython.display import Image, display

class_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']

def preprocess_image(image_path, target_size=(224, 224)):
    img = load_img(image_path, target_size=target_size)
    img_array = img_to_array(img) / 255.0
    img_array = np.expand_dims(img_array, axis=0)
    return img_array

model = tf.keras.models.load_model('/content/model.keras')

print("Upload gambar untuk diprediksi:")
uploaded = files.upload()

image_path = list(uploaded.keys())[0]

display(Image(image_path))

input_image = preprocess_image(image_path)

predictions = model.predict(input_image)

print("Predictions (Raw):", predictions)

predicted_class_index = np.argmax(predictions, axis=1)[0]
predicted_class_name = class_names[predicted_class_index]
print(f"Predicted Class Index: {predicted_class_index}")
print(f"Predicted Class Name: {predicted_class_name}")

!pip freeze > requirements.txt